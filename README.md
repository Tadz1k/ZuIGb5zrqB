![](iga_jolie.png)
*Zastosowanie technologii deepfake do podmiany twarzy Idy Świątek na twarz Angeliny Jolie

Technologia deepfake za sprawą szybkiego postępu sztucznej inteligencji i związanych z nią technik generatywnych staje się narzędziem, do którego dostęp może mieć prawie każdy. Służy ona do modyfikacji i tworzenia treści audiowizualnych przedstawiających konkretną osobę, tym samym stawiając ją w sytuacji, która mogła nigdy się nie zdarzyć. Termin “deepfake” swoją historią sięga roku 2017. Wtedy syntetyczne dane tworzone przy pomocy generatywnych sieci przeciwstawnych prezentowały głównie treści pornograficzne. Należy jednak wspomnieć jeszcze o tym, że architektura tej sieci powstała w roku 2014, a jedne z pierwszych metod modyfikowania obrazu i wideo wrzucane dzisiaj do worka z napisem “technologia deepfake” powstały dwa lata później. Wyniki manipulacji materiałów audiowizualnych jakością odstawały od tych, które widzimy dzisiaj, dlatego wtedy jeszcze o tym terminie nie było tak głośno, jak jest dziś. 

Obecnie dysponujemy na tyle rozwiniętymi technikami manipulacji treściami, że jesteśmy w stanie postawić konkretną osobę w sytuacji, która nie miała miejsca. Do tego celu trzeba dysponować średniej jakości komputerem, kilkuminutową próbką głosu, nagraniem dobrej jakości naszego celu i *voilà*! Cyberprzestępcy wykorzystują głos i wizerunek ofiary w celu zaciągania pożyczek od jej bliskich, tworzenia treści dyskredytacyjnych będących narzędziem szantażu. Często ofiarami stają się osoby publiczne, a wygenerowane treści korzystające z ich wizerunku mają na celu namawianie ludzi do ryzykownych inwestycji czy rejestracji na fałszywych giełdach kryptowalut. Deepfake'i wykorzystujące wizerunki osób publicznych mogą również mieć charakter dezinformacyjny, prowadząc do destabilizacji bezpieczeństwa społecznego lub dyskredytacji przedstawicieli państwa na scenie międzynarodowej.

Technologia deepfake to nie tylko podstawienie twarzy polityka i namawianie do ryzykownych inwestycji. Należy o tym pamiętać, szczególnie w dobie tak powszechnego używania tego rozwiązania do nieetycznych celów. Jeśli ktoś oglądał film “Irlandczyk”, a twarz Roberta De Niro wyglądała nieproporcjonalnie młodo w porównaniu do ciała - tłumaczę - stoi za tym technologia deepfake. Jeśli ktoś chce obejrzeć od nowa Matrixa, ale zamiast Keanu Reeves’a w roli głównej chciałby zobaczyć Rowana Atkinsona (aktor znany z Jasia Fasoli) to proszę bardzo. Technika modyfikacji i zamiany twarzy daje nowe narzędzia osobom odpowiedzialnym za dostarczanie nam rozrywki. Być może branża filmowa rozrośnie się o nową gałąź z dopiskiem “AI Powered”. Wówczas dostępność najpopularniejszych aktorów byłaby nieskończona, gdyż wystarczyłoby zakupić model ich twarzy, nałożyć na twarz aktora, który byłby “dawcą” ciała i zaoszczędzone pieniądze można zainwestować w lepsze efekty specjalne. Jest to wizja dość kontrowersyjna, gdyż prawdziwy kunszt aktorski i ujrzenie idola na ekranie jest czymś nieporównywalnym do wygenerowanej tożsamości, ale warto zwrócić uwagę, że daje to nowe szanse dla niskobudżetowych filmów, które mają ciekawą historię do opowiedzenia. Obrazy z wizerunkiem ukochanej osoby, która raz po raz uśmiecha się do nas brzmi trochę jak historia wyciągnięta z filmu Harry Potter, ale jesteśmy świadkami (nie od dziś, ale AI okazała się niezwykle pomocna) zjawiska, w którym technika zastępuje magię z filmów fantasy.

### Przekaz ponad barierami 

Współcześnie możemy jednak wskazać na pozytywne przejawy wykorzystania tej technologii, takich jak pokonywanie kolejnych barier komunikacyjnych. Jedną z tego typu przedsięwzięć było wytworzenie filmu promującego inicjatywę "*Malaria must die*" z udziałem David'a Beckham'a. Materiał wyróżniał się, ponieważ przekaz słynnego piłkarza został wypowiedziany naprzemiennie w 9. różnych językach. Dzięki takiemu zabiegowi, treść mogła trafić do zdecydowanie szerszej grupy odbiorców. W filmie jednak, zarówno głos jak i ruchy warg piłkarza zostały zmanipulowane przez technologię deepfake.

![[Pasted image 20240227083007.png]]
*Kadr z filmu "Malaria Must Die" z udziałem Davida Beckhama*

Innym obszarem, w którym generowanie syntetycznych danych audiowizualnych może być przydatna jest edukacja. Prezentacja realistycznych filmów, reprezentujących postacie historyczne, zgodne z ich opisami i obrazami a nawet tonem ich głosów ma potencjał reprezentowania przeszłości w nowym wymiarze, zachowując także specyfikę ubioru czy scenerii, wymagającej dzisiaj pracy wielu specjalistów. Taki sposób może prezentować minione dzieje bardziej wielowątkowo i trafiać do wyobraźni oglądających w bardziej przystępny sposób.

### Deepfake postmortem

Pozostając jeszcze na scenach Hollywood, nie sposób pominąć dokonanych już przykładów zastosowania. Głośna premiera nowego filmu "*Gwiezdne Wojny: Skywallker. Odrodzenie*" miała stanąć przed próbą zaprezentowania fabuły po tragicznej śmierci Carry Fisher, odgrywającej istotną rolę Księżniczki Lei. Reżyser podjął decyzję o manipulacji prezentowanego obrazu w taki sposób, aby pośmiertnie nadać aktorce możliwość zwieńczenia swojej roli trzy lata pod jej "oddaniu się mocy".
Takie zabiegi jednak są znacznie częstsze. Już w 2015 roku, w ramach kinowej premiery ostatni raz mogliśmy zobaczyć Braian O'conner w siódmej odsłonie "Szybkich i wściekłych".  Aktor Paul Walker, odgrywający tę rolę, odszedł tragicznie w trakcie zdjęć do filmu. Część scen, zostało nagranych po jego śmierci z nałożeniem jego twarzy i głosu. Popularnie, tego rodzaju materiały nazywane są *postmortem deepfakes*.
![[Pasted image 20240225145420.png]]
*Scena z filmu "Szybcy i wściekli 7" przedstawiająca pośmiertnie Paula Walkera (Braian O'conner)*

### Demokratyzacja technologii  

Manipulacja obrazem to zespół technik znany blisko tak długo, jak powszechny dostęp do urządzeń służących do rejestrowania takich materiałów. Techniki te jednak mocno ewoluowały dzięki nośnikom cyfrowym i szerokiemu spektrum narzędzi wspierających ten proces. Średniozaawansowana edycja obrazu nie wymaga dziś wiedzy lub dostępu do podzespołów komputerowych, przerastających możliwości przeciętnego użytkownika komputera. Zdecydowanie wyższą barierą wejścia posiada manualna edycja filmów czy dźwięku, która w branżach marketingowych czy rozrywkowych jest często z jednej strony kluczowa, a z drugiej czasochłonna. Mimo, iż jest to mniej rozbudowane, profesjonalna obróbka takich materiałów wymaga obszernej wiedzy i doświadczenia. To jednak może być niedługo przeszłość w obliczu szybkiego rozwoju sieci generatywnych.
Wystarczy porównać jakość prezentowanych efektów specjalnych wykorzystujących technologię deepfake we współczesnych filmach akcji, a w ich kultowych odpowiednikach, kiedy technologia nie była na tyle hojna. Należy założyć, iż odwrotnie proporcjonalnie do jakości takich efektów zmieniać się będą poniesione przez producentów koszty, które w świetle filmów animowanych mają zostać docelowo zredukowane o 90%. Co zostało wspomniane w rozdziale wcześniej, podobny los może czekać w przyszłości branżę filmową, w której akcje filmu wedle scenariusza wykonywane będą bez fizycznego udziału ludzi, a nawet i przy wykorzystaniu realnego wizerunku aktora. W świetle następnych kamieni milowych w generowaniu filmów, technologia staje się coraz bardziej zdolna do rzucenia tego typu wyzwania. Przytoczyć w tej kwestii można dwie premiery z lutego 2024. roku, mianowicie Sora, czyli model OpenAI generujący zaskakująco realistyczne materiały wizualne oraz nowa architektura V-Jepa od firmy Meta, zdolna w czasie rzeczywistym uczyć się filmów i "domyślać się" zasłoniętych treści, naśladując ludzki proces uczenia.
 ![[Zrzut ekranu 2024-02-25 o 14.20.04.png]]
 *Prezentacja możliwości modelu V-Jepa, zdolnego do odtwarzania zasłoniętych elementów, Źródło: https://pl.techwar.gr/342233/v-jepa-i-apantisi-tou-meta-stin-syntheti-katanoisi-vinteo/ 
 

### Fałszywa pamięć

Naukowcy ze School of Applied Psychology w University College Cork przeprowadzili badania dotyczące zjawiska nazwanego “fałszywą pamięcią” (ang. *false memory*). Badanie polegało na pokazaniu ochotnikom fragmentów popularnych filmów, z którychc część była oryginalna, a na pozostałych twarze aktorów zostały podmienione technikami deepfake. Próba badawcza wynosiła 436 osób, a celem badania było sprawdzenie, czy statyści uwierzą, że zmanipulowany film to jednak ten prawdziwy. Według opublikowanych wyników średni wskaźnik fałszywej pamięci wyniósł 49%, a wielu użytkowników zapamiętało fałszywy remake jako lepszy niż oryginalny film. Zjawisko fałszywej pamięci zostało zamieszczone w niniejszym wpisie nie bez powodu. Można założyć, że współczynnik mógłby być wyższy, jeżeli manipulacje byłyby idealne i pozbawione niespójności budzących podejrzeń. W myśl powiedzenia “kłamstwo powtarzane 100 razy staje się prawdą” i dodając do równania prędkość dystrybucji informacji w internecie niektóre manipulacje mogą nagle zlać się z prawdą, co może nieść za sobą daleko idące konsekwencje.

## Ciemna strona deepfake

Samo pojęcie deepfake, będące nawiązaniem do tworzenia nieprawdziwych treści (*"fake"*) za pomocą głębokich ("*deep*") sieci neuronowych, zostało zaobserwowane pierwszy raz w 2017 r. na platformie Reddit. Wtedy to jeden z moderatorów stworzył tzw. "subreddit", rozumiany jako temat dyskusji, w której to zaczęto publikować realne filmy pornograficzne, zawierające modyfikację twarzy występujących w filmach aktorów na podobizny celebrytów. W pewnym sensie, poprawnym jest stwierdzenie, iż etymologia słowa deepfake jest bezpośrednio związana z pornografią...
Niestety, zainteresowanie wykorzystania technik takich jak zamiana twarzy nie straciła na popularności w perspektywie nieprzyzwoitych treści, nadając mu nowy wymiar. Wedle raportu *"2023 State of Deepfakes"* sporządzonego przez *Home Security Heroes*, od 2019 do 2023 roku ilośc tego typu filmów dostępnych online wzrosła o 550%. Pornografia stanowi zawrotne 98% wszystkich zmodyfikowanych filmów dostępnych w sieci. Zgodnie z przedstawionym w raporcie wnioskom, treści te są niemal w całości wykorzystywane przeciwko kobietom, jako sposób szantażu czy zniesławienia w celu zniszczenia kariery zawodowej czy napaści seksualnej. Wskazano nawet, iż w jednym z hiszpańskich miast, 30 dziewcząt w wieku od 12 do 14 lat narażono na rozprzestrzenienie ich sfalsyfikowanych zdjęć pornograficznych. Co gorsza nie jest to odosobniony przypadek w skali globalnej. Takie działania mogą bardzo poważnie wpłynąć na stan psychiczny kobiet dotkniętych tego typu przestępstwami, a także wzmagają niepokój społeczny, ze względu na rosnącą prostotę przeprowadzania takich manipulacji.

![[Pasted image 20240225185246.png]]
*Informacje statystyczne dotyczące rozpowszechnienia i oglądalności materiałów deepfake wskazane w raporcie "2023 State of Deepfakes".*

## Cyberprzestępstwa

Co opisano wyżej, technologia umożliwiająca tworzenie materiałów syntetycznych jest wykorzystywana jako narzędzie w filmografii, edukacji czy też sztuce cyfrowej, pozwalając na wspomaganie procesu wytwarzania treści. Materiały takie mogą być jednak wykorzystane również do manipulacji, oszustw, czy naruszania prywatności. 
Cyberprzestępczość nasila się na całym świecie, a technologie sztucznej inteligencji, wspierają oszustów w kradzieży tożsamości, wykorzystując ją do szerzenia dezinformacji m.in. poprzez tworzenie rzeczywistych wizerunków i głosów. Dzięki takim zabiegom możliwym stało się nakłonienie pracownika międzynarodowej korporacji, z oddziałem w Hongkongu, do wypłacenia 25 milionów USD oszustom. Mimo początkowych podejrzeń pracownika co do otrzymanych mailowo instrukcji, oszuści zorganizowali wideorozmowę, w której wszyscy uczestnicy podszywali się pod pracowników organizacji z otoczenia ofiary. To uśpiło jej czujność i przełamało opory w spełnieniu  oczekiwań swojego "dyrektora finansowego". Podobna historia miała miejsce w firmie energetycznej z Wielkiej Brytanii, w której pracownik przelał 220 000 EUR na podstawie instrukcji otrzymanej od rzekomego szefa, z perfekcyjnie naśladowanym głosem i akcentem.

https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html
https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-deepfake-was-used-to-scam-a-ceo-out-of-243000/

Zdarzają się przypadki, w których oszuści wykorzystują zmanipulowane materiały, z udziałem znanych osób, aby promować różnego rodzaju oszustwa, zwane potocznie "*scamami*". Takie materiały mogą prezentować celebrytów, liderów biznesu lub inne znane postacie, które rzekomo popierają produkty inwestycyjne, akcje związane kryptoaktywami lub inne schematy zarobkowe. W rzeczywistości, te persony nigdy nie udzieliły swojej zgody na użycie ich wizerunku, a materiały jest sfałszowane, co jest szeroko identyfikowane również w polskich realiach. 
W październiku 2023r. na fałszywym kanale na Facebooku pojawił się materiał wykorzystujący wizerunek Roberta Lewandowskiego, promujący szybkie zarabianie pieniędzy za pomocą aplikacji mobilnej. Podkreślano, że wykorzystywana przez piłkarza aplikacja bez wysiłku generuje przychód 500 EUR miesięcznie. Innym tego typu procederem były rzekome programy inwestycyjne Baltic Pipe Project, uruchomionego przez polski rząd, który "*gwarantuje wszystkie wypłaty*". Reklamy te wykorzystywały wizerunki polskich polityków, m.in. Mateusza Morawieckiego, Andrzeja Dudy, Jarosława Kaczyńskiego. Nie były to akcje przedstawiane wyłącznie w formie postów na portalach społecznościowych, ale także reklamy wyświetlane w wielu innych przestrzeniach internetowych.
https://niebezpiecznik.pl/post/uwaga-na-deepfakei-z-vip-ami-np-andrzejem-duda-robertem-lewandowskim/

![[Lewy_DF.mp4]]
*Fałszywa reklama aplikacji do inwestycji, wykorzystująca wizerunek znanego piłkarza polskiego Roberta Lewandowskiego z wygenerowanym materiałem głosowym.*

Również powszechne stały się tzw. "*farmy trolli*", czyli zorganizowane grupy, tworzące fałszywe konta na mediach społecznościowych. Wykorzystują one technologię deepfake do nadawania większej wiarygodności swoim działaniom, poprzez tworzenie realistycznych zdjęć i nagrań, imitując autentyczne konta. Te konta są wykorzystywane do manipulowania opinią publiczną, rozsiewania dezinformacji oraz wpływania na dyskurs polityczny i społeczny.

https://cyberscoop.com/fbi-foreign-actors-deepfakes-cyber-influence-operations/

Uważa się, że jednym z głównych zagrożeń w 2024 roku będą polityczne ataki deepfake, które zalicza się do najbardziej ogólnego obszaru zagrożeń społecznych. Będzie to widoczne zarówno w formacie globalnym w trakcie wyborów prezydenckich w USA, a także możliwe do zaobserwowania w trakcie wyborów samorządowych w Polsce lub szerzej do Parlamentu Europejskiego. To wyzwanie które niesie ogólny rozwój technologiczny, na które należy odpowiedzieć. Problemem wolnego świata, o zgrozo, jest tutaj identyfikacja tego typu treści, nie godząca w wolność słowa. Taka postawa, z natury szlachetna, spowalnia jednak reakcje, zgodnie z powiedzeniem, że "*kłamstwa podróżują po całym świecie, zanim prawda zdąży założyć buty*".

## Rodzaje wizualnych deepfake
### Zmiana tożsamości (identity swap)

![[cruisemusk.png]]

Technika podmiany tożsamości pozwala na przeprowadzenie transferu twarzy, bądź całej głowy jednej osoby na ciało innej. Jest to proces, który może dostarczyć bardzo wiarygodne rezultaty, szczególnie, gdy fryzura i rysy twarzy osoby, której tożsamość jest przenoszona (tzw. źródło) są podobne do tych, na której ma zostać przeprowadzona zamiana (tzw. cel). Wówczas trudność identyfikacji takiego materiału jako fałszywy staje się jeszcze większa.
### Odtwarzanie twarzy (face reenactment)

![[face_animation.gif]]

Technika ta polega na odtworzeniu mimiki, ekspresji twarzy na statycznym, nieruchomym obrazku na podstawie nagrania referencyjnego. Dzięki temu możemy uzyskać nagrania wypowiedzi, bądź ruchu twarzy osoby docelowej posiadając tylko jedno, dobrej jakości zdjęcie przedstawiające tę osobę.
### Przekształcanie dźwięku w ruch ust (wav to lip) 

![[musk_wav2lip 1.png]]

W ostatnich latach zdobyła ogromną popularność innowacyjna metoda znana jako "*wav to lip*", która rewolucjonizuje sposób, w jaki postrzegamy i interpretujemy materiały wideo. Idea jest prosta, ale efekty zaskakująco realistyczne: technologia ta synchronizuje ruchy ust osoby widocznej na ekranie z dźwiękiem, który podano jako wejście. W praktyce oznacza to, że widzowie otrzymują wrażenie, że osoba na ekranie faktycznie wypowiada słowa, które słychać w nagraniu audio. Efekt jest tak przekonujący, że powstają wrażenie autentycznych wypowiedzi, choć w rzeczywistości są one wytworem sztucznej manipulacji. Dzięki wykorzystaniu zaawansowanej syntezy dźwięku, nagrania te zdają się być niezwykle realistyczne, co stwarza nowe wyzwania w kontekście autentyczności prezentowanych treści w erze cyfrowej.
### Modyfikacja atrybutów twarzy (face attributes modification)

![[cruise_bearded.png]]

Jest to proces zmiany wyglądu twarzy na materiałach wizualnych. Pozwala na modyfikację koloru oczu, owłosienia, zmianę mimiki, korektę uzębienia i cery, oraz wiele więcej. Metoda szczególnie popularna w branży beauty, gdyż umożliwia wizualizację potencjalnych wyników zabiegów przed podjęciem działania. Ostatnimi czasy technika ta znajduje swoje zastosowania w aplikacjach służących do przeprowadzania spotkań biznesowych on-line, sprawiając wrażenie utrzymywania ciągłego kontaktu wzrokowego.
### Łączenie twarzy (face morph)

![[elon_tom_morph.png]]

Metoda face morph polega na wykorzystaniu zdjęć twarzy dwóch różnych osób. Efektem jej działania jest nowe zdjęcie twarzy, które przedstawia osobę fikcyjną posiadającą cechy wyglądu obu osób, których wizerunki zostały użyte do jego stworzenia. Niestety, przestępcy często wykorzystują tę technikę do podszywania się pod inne osoby, tworząc fałszywe dowody tożsamości. Na takim zdjęciu można zobaczyć jednocześnie ofiarę (czyli osobę, pod którą ktoś się podszywa) oraz drugą osobę, która próbuje wykorzystać cudzy wizerunek. Co gorsza, systemy biometrycznej weryfikacji o niskiej jakości mogą zostać łatwo oszukane za pomocą takich fałszywych obrazów.
### Syntetyczna twarz (fully synthetic face)

![[synthetic.png]]

Twarze w pełni syntetyczne przedstawiają wizerunki osób, które nie istnieją. Ewentualne podobieństwo jest tutaj przypadkowe. Algorytm, który otrzymał do analizy wiele różnych wizerunków "nauczył się", jak wygląda ludzka twarz i jest w stanie tworzyć wizualizacje o losowych atrybutach wyglądu. 
Istnieje również opcja tworzenia syntetycznych twarzy na podstawie zdjęcia referencyjnego. Wówczas wygenerowany obraz przypomina osobę z przykładu referencyjnego. Model sztucznej inteligencji szuka wówczas podobizn posiadających jak najwięcej wspólnych atrybutów ze źródłem.

![[gan_generated.png]]

## "Jak to jest zrobione?" - o tworzeniu wizualnego deepfake

Tworzenie zmanipulowanego materiału wizualnego może odbywać się na wiele różnych sposobów, a ilość zasobów potrzebnych do wykonania deepfake daną techniką jest zwykle proporcjonalna do jakości wyników. Do prostych manipulacji mogą wystarczyć dwa zdjęcia: **celu** - ciała, do którego wklejamy twarz oraz  **źródła**, czyli twarz, którą chcemy wstawić. 
Inne, bardziej efektowne metody wymagają analizy dziesiątek tysięcy zdjęć wybranej osoby, na których uchwycono zróżnicowane ustawienie twarzy, mimikę i ekspresję twarzy oraz reakcje skóry na różne warunki środowiskowe, takie jak oświetlenie czy wpływ temperatury – na przykład pot, zmiany w odbiciu światła od skóry czy zaczerwienienie.

Do tworzenia deepfake wykorzystywane są również metody wykrywania punktów twarzy, które składają się na jej mapę, pozwalając komputerowi na odpowiedni dobór rotacji i skali maski do twarzy, aby ta jak najlepiej mogła wkomponowywać się w dany obraz.
  

![[Pasted image 20240227083217.png]]
*Proces nakładania maski twarzy Angeliny Jolie na zdjęcie przedstawiające Igę Świątek.*

Deepfake’i najwyższej jakości, które z założenia mają być nieodróżnialne od rzeczywistych zdjęć zwykle poddawane są post-processingowi przez człowieka. To znaczy osoba o umiejętnościach graficznych poprawia wszelkie niedociągłości, wygładza granice maski, poprawia przejścia gradientu tak, aby materiał był jak najtrudniejszy do zidentyfikowania jako zmanipulowany. 
## Audio deepfake

Generowanie syntetycznych głosów, znanych również jako audio deepfake’i dzieli się na na dwa podejścia: konwersję tekstu na mowę (ang. *Text-To-Speech*, TTS), oraz klonowanie głosu (ang. *voice cloning*, VC).

Systemy syntezy głosu Text-to-Speech (TTS) oparte na sieciach neuronowych działają poprzez przekształcanie wprowadzanego tekstu w syntetyczną mowę. Proces zaczyna się od analizy i przekształcanie tekstu na fonetyczne elementy, które są następnie interpretowane przez algorytmy lingwistyczne. Algorytmy te odzwierciedlają intonację i akcenty potrzebne do uzyskania naturalnie brzmiącej mowy. Kolejnym etapem jest wyciągnięcie cech akustycznych człowieka. W procesie wytwarzania mowy człowieka biorą udział różne części anatomiczne, takie jak jama nosowa, podniebienie, język, wargi, żuchwa, a także strumień powietrza z płuc. Częstotliwości rezonansowe, które powstają, gdy powietrze przechodzi przez tor akustyczny tworzy dźwięk, zawierający unikalne właściwości głosu każdej osoby. Z zapisu dźwięku, wydobywa się unikalne cechy dla każdego człowieka tworząc embedding cech akustycznych. W ostatnim kroku cechy akustyczne danego człowieka są “łączone” z cechami fonetycznymi tekstu, pozwalając na generowanie realistycznie brzmiącego głosu. Elementem odpowiedzialnym za ten proces jest moduł kodowania głosu, zwany vocoderem (Voice + coder). Vocoder syntezuje fale dźwiękowe, będące zapisem cyfrowym sygnałów audio, które mogą być następnie odtwarzane jako mowa. Odpowiada za kształtowanie brzmienia mowy, nadając jej charakterystyczne cechy głosowe danego człowieka.

![[Pasted image 20240225084136.png]]*Uproszczony schemat działania systemu TTS. Moduł generuje mowę na podstawie dostarczonej treści i próbki głosowej danego człowieka.*

Klonowanie głosu, to bardziej zaawansowana technologia, która pozwala na tworzenie cyfrowych kopii głosów ludzi. Systemy VC to zaawansowane techniki przetwarzania sygnałów audio, które pozwalają na przeniesienie indywidualnych charakterystyk głosowych jednej osoby - mówcy źródłowego A do generowania syntetycznej mowy za pomocą innego mówcy - mówcy docelowego, który dostarcza treść i sposób mowy. Proces klonowania głosu rozpoczyna się od ekstrakcji indywidualnych cech głosowych z próbek audio źródłowego mówcy A. Te cechy, nazywane również *embeddingami*, czyli esencją tego, co sprawia, że każdy głos jest wyjątkowy. Następnie system VC wykorzystuje te *embeddingi* do wygenerowania mowy, która ma cechy charakterystyczne dla głosu źródłowego mówcy, ale zawiera treść wypowiedzi docelowego mówcy B. Systemy te umożliwiają nie tylko reprodukcję tonu, modulacji i akcentu danego człowieka, ale także pozwalają naśladować emocje i sposób mówienia. 

![[Pasted image 20240225084554.png]]
  *Uproszczony schemat działania systemu VC. Moduł ten generuje syntetyczny głos na podstawie cech akustycznych mówcy źródłowego oraz treści i sposobu mówienia docelowego mówcy (fałszerza)*


Nowoczesna technologia pozwala na tworzenie dobrej jakości syntetycznych materiałów audio bazując tylko na minucie głosu w przypadku metod TTS i około 6 minut dla podejścia klonowania głosu. Poniżej zamieszczone są przykłady syntetycznej mowy wygenerowane metodą TTS, wykorzystującą komercyjne oprogramowanie ElevenLabs oraz VC, uzyskane narzędziem RVC o otwartym kodzie źródłowym.

![[11Labs_EBT_TTS.mp3]]*Przykład treści wygenerowanej na podstawie tekstu i próbki głosu o długości około 60 sekund.*

![[rvc_EBT_VC.wav]]*Przykład syntetycznej mowy uczonej na próbce głosu o długości 6 minut.*

## Jak rozpoznać deepfake?

### Czy człowiek potrafi wykryć deepfake?
  
Rozpoznanie deepfake przez człowieka jest zadaniem niezwykle trudnym, głównie z powodu coraz wyższej jakości i przekonującego charakteru tych materiałów. W listopadzie 2023 ukazał się artykuł, który przedstawia badanie oceny umiejętności ludzi w identyfikowaniu wysokiej jakości deepfake'ów w zestawie filmów. Autorzy pracy wykazali, że w środowisku naturalnym, bez wcześniejszych ostrzeżeń o potencjalnej obecności fałszywych treści, osoby narażone na neutralnie tematyczne fałszywe filmy nie wykazywały większej zdolności do wykrywania anomalii w porównaniu z grupą kontrolną, która miała dostęp jedynie do autentycznych filmów. Dodatkowo, po zastosowaniu ostrzeżenia, że wśród pięciu filmów może znajdować się przynajmniej jeden deepfake, jedynie 21,6% respondentów skutecznie zidentyfikowało deepfake jako jedyny nieautentyczny film, natomiast reszta błędnie uznała co najmniej jeden z autentycznych filmów za fałszywy.

Podobne badania zostały przeprowadzone na danych audio. Naukowcy z University College London sprawdzali, w jakim stopniu ludzie są w stanie odróżnić autentyczną mowę człowieka od sztucznie wygenerowanych komunikatów przy użyciu technik deepfake. Okazało się, że mowę wygenerowaną za pomocą AI uczestnicy badania poprawnie identyfikowali tylko w 73% przypadków. Osoby, które udzieliły poprawnej odpowiedzi, zauważały, że ich uwagę w nagraniach zwracał oddech, odstępy między słowami oraz płynność mówienia. Wynik ten poprawił się jedynie nieznacznie wśród uczestników badania, którym przedstawiono, jakie cechy mogą wskazywać na sztucznie wygenerowaną mowę, a następnie odtworzono im kilka nagrań, informując, że są to deepfake'i.

### Wykrywanie syntetycznych głosów

Audio deepfake’i obecnie wciąż zawierają wiele niedoskonałości. W obliczu materiałów budzących podejrzenia lub w trakcie rozmów telefonicznych o charakterze socjotechnicznym, które grają na emocjach, warto zwrócić uwagę na nietypowe przerwy w mowie, które mogą wskazywać na sztuczne spajanie fragmentów wypowiedzi. Innym wskaźnikiem są nienaturalne intonacje, często rozbieżne z emocjami, które powinny towarzyszyć wypowiedzi.  W trakcie nagrania lub wypowiedzi może dojść do nieoczekiwanej zmiany barwy głosu, tonu, wysokości dźwięku, lub akcentu. Metody syntezy głosu czasem mają problem z utrzymaniem konsekwentnie określonego profilu głosowego. Dodatkowym sygnałem mogą być błędy w wymowie słów, przede wszystkim liczb, dat, nazwisk, terminów technicznych i słów obcojęzycznych. Przykład takiej sytuacji obserwujemy w deepfake'u z udziałem M. Morawieckiego, gdzie jego wizerunek został wykorzystany do promocji fałszywej inwestycji (dodać). Można tam usłyszeć niezręczne sformułowania typu: 
"*od 1 września 2023 (dwutysięcznego dwadzieścia trzeciego roku) uruchamiamy platformę, dzięki której każdy Polak będzie zarabiał (...)*", które nie są charakterystyczne dla naturalnej mowy i mogą wzbudzać podejrzenia co do autentyczności nagrania.

![[1695113477045.mp4]]
  *Przykład deepfake'a umieszczonego na Facebooku promującego fałszywą inwestycję finansową.*

Cechą nabytą przez ludzi jest sposób mówienia. Jeśli słyszany głos jest bardzo podobny do osoby, którą znacie, znakiem ostrzegawczym może być inny styl mówienia i tempo mowy, niepasujące do danego mówcy. Naturalne nagrania rzadko są wolne od odgłosów w tle. Ich brak, niespójność lub trzaski może sugerować, że mamy doczynienie z generowaną mową. 

Warto wspomnieć, że metody generowania głosu stają się coraz bardziej idealne, praktycznie nierozróżnialne zmysłami przez człowieka. W takich przypadkach niezbędna może się okazać analiza spektrogramów, która może ujawnić artefakty związane z cyfrowym przetwarzaniem sygnałów i metod generatywnych. 

### Wykrywanie manipulacji w obrazie twarzy

W przypadku manipulacji wizerunkiem twarzy na zdjęciach lub materiałach wideo również mogą wystąpić błędy. Łatwo dostrzegalne są artefakty, które pojawiają się przy większym obrocie głowy. Jak już wspomniano, manipulacje obrazem twarzy zwykle opierają się na jej punktach charakterystycznych, takich jak oczy, nos, usta. Przy większych obrotach maski punktów wykorzystywane do mapowania twarzy mogą nie nadążać za zmianą perspektywy, co prowadzi do do widocznych zniekształceń. Kiedy głowa na wideo się obraca lub przechyla, materiały deepfake mogą wykazywać nienaturalne artefakty, takie jak zniekształcenia skóry lub niewłaściwe śledzenie ruchów. Podobna sytuacja występuje przy zasłonięciach obszaru twarzy, np. przy przesuwaniu dłonią, lub innym obiektem przed twarzą. W miejscach granic nakładania twarzy, mogą także wystąpić nienaturalne cienie lub rozmazania, i inne błędy na krawędziach konturu twarzy. 

![[image191 1.gif]]

W przypadku syntetycznych materiałów wideo z wykorzystaniem synchronizacji mowy, ruch warg często ma problemy z synchronizacją dźwięku. Może być to widoczne jako opóźnienie lub przedwczesne ruchy warg słyszalnych słów. Dodatkowo, szczegóły takie jak zęby, kontur ust, oczy mogą wydawać się nienaturalnie rozmazane lub nierealistycznie ostre, zwłaszcza, gdy osoba na nagraniu zmienia wyraz twarzy. W materiałach deepfake, mruganie może być rzadkie, nieregularne lub w ogóle nie występować. 

![[image193 2.gif]]

Często występują także subtelne, ale niezgodne z rzeczywistością zmiany w proporcjach twarzy, które mogą się pojawiać, lub zanikać w trakcie nagrania. W naturalnych nagraniach emocje są spójne z tonem głosu, mimiką twarzy i ogólnym zachowaniem. W deepfake’ach ta spójność jest często zaburzona. Szczególnie trudne do realistycznego odtworzenia są szczegóły takie jak włosy, zmarszczki, zęby. Błędy w ich renderowaniu, zmiany, przerwy, zakrzywienia mogą także być oznaką manipulacji. W materiałach wizualnych często pojawiają się, różnego rodzaju błędy generacji. Obejmują one wszelkie niespójności w renderowaniu, które nie pasują do oczekiwanego naturalnego obrazu, zarówno w obrębie twarzy jak i towarzyszącego tła. 

![[bledy_generacji.png]]
*Przykładowe zdjęcia zawierające błędy generacji. Od lewej: a) ślady okularów, artefakty na zębach, b) nieciągłości we włosach, artefakty w okolicy uszu, c) niespójne dłonie.*


Metody generatywne, takie jak sieci GAN (ang. Generative Adversarial Networks) i modele dyfuzyjne (Stable Diffusion Models) pozostawiają charakterystyczne ślady, lub cechy, które można wykrywać za pomocą technik przetwarzania obrazu, jak i technik wizji komputerowej. Cechy te nazywamy *fingerprintami*, mogą obejmować powtarzające się tekstury, szum, nienaturalne wzory lub inne regularności obszarowe, które zazwyczaj nie są spotykane w autentycznych materiałach.

Warto zwrócić uwagę, że wyrywanie deepfake’ów to nie tylko wyzwanie technologiczne, ale też kwestia uważności. Eksperci cyberbezpieczeństwa i uczenia maszynowego ciągle rozwijają metody i narzędzia wspierające automatyczne wykrywanie manipulacji.

## Wykrywanie deepfake, a kompresja video

W rzeczywistości, w której wszyscy jesteśmy otoczeni fałszywymi informacjami, musimy być stale w trybie gotowości, a przy okazji stajemy się podejrzliwi wobec każdego materiału. 

Popularne media społecznościowe używają metod kompresji plików, aby te, przechowywane na serwerach nie generowały podwyższonych kosztów związanych z opłaceniem przestrzeni dyskowej. W związku z tym dane poddawane są procesom, które mają na celu zmniejszyć ich wielkość, zachowując przy tym przenoszoną informację. Kompresja może w efekcie przyczynić się do powstania nowych artefaktów na obrazie, które w konsekwencji wzbudzą ograniczone zaufanie wobec materiału. Poniżej znajdują się przykłady, które ukazują klatkę z materiału wideo poddanego kompresji  z zaznaczeniem powstałych artefaktów (lewa strona) i klatkę oryginalną, pochodzącą z oryginalnego pliku wideo o wysokiej rozdzielczości.

  
| ![[Pasted image 20240227075516.png]] | ![[Pasted image 20240227075723.png]] |
| ------------------------------------ | ------------------------------------ |
*Porównanie materiałów - otwarcie ust.*


W okolicy kołnierza kolor skóry zaczął mieszać się z kolorem koszuli. Efektem tego powstał defekt - koszula przejmująca kolor skóry.  
  
| ![[Pasted image 20240227075648.png]] | ![[Pasted image 20240227075709.png]] |
| ------------------------------------ | ------------------------------------ |
 *Porównanie materiałów - przysłonięcie twarzy.*

Między małym palcem, a serdecznym powstała plama o niebieskim kolorze, a koniec palca środkowego został rozmazany. Może to sugerować konsekwencje przesłonięcia twarzy i błędnego dostosowania maski do twarzy, lecz jest to efekt kompresji, a materiał jest prawdziwy.

| ![[Pasted image 20240227080125.png]] | ![[Pasted image 20240227080213.png]] |
| ------------------------------------ | ------------------------------------ |
*Porównanie materiałów - otwarcie ust i rotacja głowy.*

Granica między twarzą, a tłem stała się bardzo ostra, zawiera “proste linie” i wygląda jak błąd. Dodatkowo okolice brwi wyglądają na rozmazane, a sama brew sprawia wrażenie “odklejającej się”. Na karku odstające włosy wyglądają na niepoprawnie wygenerowane i nie zachowują ciągłości.

## Implikacje związane z syntetyzowaniem danych

W czasach syntetycznych danych, sztucznej inteligencji i wirtualnej rzeczywistości (VR) człowiek stoi na granicy dwóch światów - rzeczywistego i wirtualnego, balansując między nimi. Bywają sytuacje, w których niektóre jednostki cenią sobie dużo bardziej świat wirtualny, niż ten prawdziwy. Pomijając problem zacierania się granicy pomiędzy tymi dwoma światami, uzależnieniami od wirtualnej rzeczywistości, w kontekście niniejszego artykułu należy podnieść jeszcze jeden problem, czyli intencjonalne wprowadzanie człowieka w błąd. 

### Antropomorfizacja skryptów

Przy tak dynamicznym rozwoju sztucznej inteligencji pojawiają się problemy natury etycznej. Jednym z nich jest zagadnienie, kiedy można stwierdzić, że dany program komputerowy jest na tyle zaawansowany, aby mógł uzyskać obywatelstwo, decydować za siebie i być traktowany jako indywidualna jednostka. W dobie niezwykle "ludzkich" odpowiedzi pochodzących z wielkich modeli językowych, projektów technicznych gigantów, które są w stanie "widzieć, słyszeć i odpowiadać" problem ten wydaje się być coraz bardziej palący. W tym miejscu należy wspomnieć, że do osiągnięcia poziomu samoświadomości przez program komputerowy jeszcze daleko. Sztuczna inteligencja to zwykły algorytm, który na podstawie przedstawianych mu danych potrafi generalizować dane, które otrzyma i na tej podstawie stworzyć odpowiedź, która jest w stanie zaimponować człowiekowi przez niezwykle precyzyjne odtworzenie rzeczywistości.

Chatboty to jeden z najbardziej interaktywnych przykładów zastosowania sztucznej inteligencji. Każdy użytkownik może włączyć przeglądarkę, otworzyć chat i rozpocząć dyskusję z komputerem. Chatboty mają często nadawane cechy płciowe, a ich odpowiedzi bywają wielowątkowe i swoją formą przypominają konwersację z prawdziwym rozmówcą. Przyczynia się to do rozwoju relacji między programem komputerowym, a człowiekiem. Wówczas algorytm nie jest tylko narzędziem komputerowym, a kompanem, któremu powierza się swoje sekrety, zadaje pytania o opinie itp. Zjawisko to znane jest jako Efekt Elizy, którego początki sięgają 1996 roku, kiedy powstał program ELIZA, imitujący zachowanie psychoterapeuty. Przeprowadzone badania wykazały, że ludzie łatwo nawiązują emocjonalne więzi z takimi programami. 

![[Pasted image 20240225143001.png]]

Od 1996 roku technologia syntetyzowania głosu, obrazu i generowania tekstu znacznie ewoluowała, ułatwiając nawiązywanie interakcji i ludzi z maszynami. Współczesne reklamy widziane w internecie stale zachęcają do pobierania aplikacji "przyjaciela AI", które oferują nie tylko towarzystwo do konwersacji, ale także umożliwiają nawiązywanie głębszych, romantycznych więzi określanych mianem "Relacji 5.0". 

### Gwiazdy AI

Branże showbiznesu, pornograficzne czy influencerskie chętnie wykorzystują potencjał syntetyzowania danych. Na platformach społecznościowych, jak Instagram, można znaleźć profile poświęcone wirtualnym postaciom, które nie istnieją w rzeczywistości, są wygenerowanymi tworami, a przyciągają rzesze obserwujących. 

Jednym z najpopularniejszych kont jest @lilmiquela. Profil ten zgromadził ponad 2.5 miliona obserwujących, identyfikując się jako 19-letni robot mieszkający w Los Angeles i wspierający ruch #BlackLivesMatter. W komentarzach pod postami @lilmiquela można znaleźć komplementy, życzenia świąteczne, a nawet pytania o jej "robotyczną" naturę.

Poza Instagramem i jawnym sygnowaniem profili jako “robot”, albo “AI powered” nie brakuje również profili, które kreują się na prawdziwych ludzi na platformach takich jak OnlyFans. Nierzadko użytkownicy płacą takiemu “avatarowi” za zdjęcia - na przykład rozbierane nie podejrzewając nawet, że taka osoba nie istnieje, a zdjęcia są stworzone przez komputer. W kwietniu 2023 roku, na platformie Reddit pojawił się wpis użytkownika, który poczuł się oszukany, odkrywając, że przeznaczył swoje pieniądze na profil, za którym nie stoi rzeczywista osoba, lecz oszust z komputerowo wygenerowanymi materiałami kobiety.

W sferze rozrywki wykorzystuje się sztuczną inteligencję do tworzenia syntetycznych kopii influencerów, które interaktywnie angażują ich fanów. Algorytmy uczenia maszynowego, zaopatrzone w próbki głosu i obrazy celebryty, pozwalają na tworzenie wiernych kopii, które wypowiadają się i wyglądają jak oryginał. Fanów zachęca się do nabycia dostępu do takiego systemu, który pozwala im na prowadzenie wirtualnej komunikacji z "sztuczną kopią" ich idola.

## Podsumowanie

Rzeczywistość w dobie napędzanej AI stawia ludzkość w niespotykanym dotąd momencie, w którym każda jednostka może w przenośni stać się światowej klasy iluzjonistą, a takie iluzjonistyczne umiejętności, a będąc niepowołanych rękach rodzą globalne ryzyka. Zwłaszcza w połączeniu z aktualnym poziomem skomunikowania ludzkości. Jesteśmy momencie, w którym zagrożenia ze strony deepfake są nieznane szerszemu gronu potencjalnych ofiar, przez co jest ona nęcąca dla oszustów, mających świadomość wykładniczego rozwoju tej technologii w przyszłości. Należy przewidywać, iż momentem kulminacyjnym będzie tworzenie pełnometrażowych materiałów, których człowiek nie będzie wstanie rozróżnić od rzeczywistości, co może mieć katastrofalne skutki dla percepcji jednostki, grup społecznych czy narodów. Najbardziej zagrożone w tym obszarze są kraje wolnego świata, w których tego typu szum rozchodzi się najłatwiej, a technologia taka jak automatyczna klasyfikacja materiałów syntetycznych na ten moment nie spełnia oczekiwań co do zapewnienia bezpieczeństwa. Wprowadza nas to w posępną wizję konkursu zbrojeń obydwu stron, gdzie pierwsza ze stron konfliktu skupi się na wytwarzaniu iluzji, nowych technik i sposobów ich wykorzystania, a druga będzie się przed nimi bronić, identyfikując je i przeciwdziałać ich wpływowi. Aktualnie obserwujemy połowę pierwszego aktu teatru działań deepfake, przypominający niejako pantomimę. Należy mieć nadzieję, że w ostatniej scenie to świat realny pozostanie zwycięzcą.


Źródła:
https://www.humanprotocol.org/blog/what-is-the-eliza-effect-or-the-art-of-falling-in-love-with-an-ai

https://techxplore.com/news/2023-04-sex-companionship-ai-human-machine-relationships.html

https://eightify.app/summary/artificial-intelligence-and-technology/bella-influencer-clones-herself-with-ai-and-sells-out

https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained  

https://www.researchgate.net/publication/361086563_Deepfakes_generation_and_detection_state-of-the-art_open_challenges_countermeasures_and_way_forward

W 2017 roku na Redit zaczął wrzucać posty porno: https://www.britannica.com/technology/deepfake
2023 State of Deepfakes: https://www.pcmag.com/news/the-internet-is-full-of-deepfakes-and-most-of-them-are-porn
[https://www.irishexaminer.com/news/arid-41006312.html](https://www.irishexaminer.com/news/arid-41006312.html)
Andrew Lewis, Patrick Vu, Raymond M. Duch, Areeq Chowdhury, Deepfake detection with and without content warnings, https://doi.org/10.1098/rsos.231214, 27.11.2023
Carry Fisher / Star Wars / Księżniczka Leia https://expmag.com/2020/10/a-new-film-with-a-dead-star-thanks-to-cgi-and-deepfake-technology-its-possible/
in 2019’s _Star Wars: The Rise Of Skywalker,_
https://www.hollywoodreporter.com/movies/movie-news/how-furious-7-brought-late-845763/
[Paul Walker](https://www.hollywoodreporter.com/t/paul-walker/)
https://wustllawreview.org/2023/02/22/a-privacy-torts-solution-to-postmortem-deepfakes/
https://www.expresscomputer.in/guest-blogs/future-of-deepfake-in-education-and-employability-sector/106735/
Malaria must die, Backham: https://www.youtube.com/watch?v=QiiSAvKJIHo&ab_channel=ZeroMalariaBritain
 Cięcie kosztów animacji: https://www.bloomberg.com/news/articles/2023-11-09/ai-will-cut-cost-of-animated-films-by-90-jeff-katzenberg-says?embedded-checkout=true
Sora: https://openai.com/sora
V-Jepa: https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/

Ikony pochodzą z https://www.flaticon.com/


![]([https://drive.google.com/file/d/18v7G96z1WDDMN8GgYSVhb-Q0ABSRO9BQ/view?usp=sharing](https://lh3.googleusercontent.com/drive-viewer/AKGpihbBKntHAtuBJoq7stn6vDRCSx2LdFB6WQs_xOnAGgtKk30_fRxrm7cvT2ISdhrCuqcVr1GGPffmb5IDmvOFHPq7858JGA=s2560)https://lh3.googleusercontent.com/drive-viewer/AKGpihbBKntHAtuBJoq7stn6vDRCSx2LdFB6WQs_xOnAGgtKk30_fRxrm7cvT2ISdhrCuqcVr1GGPffmb5IDmvOFHPq7858JGA=s2560)
